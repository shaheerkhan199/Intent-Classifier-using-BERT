{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model Saving.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "io9JRSgM6v0U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6ffe585c-b656-4baa-f94a-5c6905772d79"
      },
      "source": [
        "!pip install -q tf-nightly"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 390.3MB 39kB/s \n",
            "\u001b[K     |████████████████████████████████| 460kB 47.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.2MB 38.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfEUm1iJ6xQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "8e3e7114-fa6d-453d-c15f-92d084409d54"
      },
      "source": [
        "!pip install -q tf-models-nightly"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.0MB 2.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 18.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 36.6MB 1.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 47.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 276kB 45.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 48.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 10.7MB/s \n",
            "\u001b[?25h  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO76XtZS7BhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "from official.modeling import tf_utils\n",
        "from official import nlp\n",
        "from official.nlp import bert\n",
        "\n",
        "# Load the required submodules\n",
        "import official.nlp.optimization\n",
        "import official.nlp.bert.bert_models\n",
        "import official.nlp.bert.configs\n",
        "import official.nlp.bert.run_classifier\n",
        "import official.nlp.bert.tokenization\n",
        "import official.nlp.data.classifier_data_lib\n",
        "import official.nlp.modeling.losses\n",
        "import official.nlp.modeling.models\n",
        "import official.nlp.modeling.networks"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms9yXJhm7F6j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "802b675f-a33b-4149-8b94-2738a5653131"
      },
      "source": [
        "# configuration files for BERT model\n",
        "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12\"\n",
        "tf.io.gfile.listdir(gs_folder_bert)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bert_config.json',\n",
              " 'bert_model.ckpt.data-00000-of-00001',\n",
              " 'bert_model.ckpt.index',\n",
              " 'vocab.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB0qB4T_7OHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hub_url_bert = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUT7YYOz7Qs1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3fc2a1aa-1939-4086-e656-517f230cbf0c"
      },
      "source": [
        "# setting up tokenizer and model\n",
        "# Set up tokenizer to generate Tensorflow dataset\n",
        "tokenizer = bert.tokenization.FullTokenizer(\n",
        "    vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\n",
        "     do_lower_case=True)\n",
        "\n",
        "print(\"Vocab size:\", len(tokenizer.vocab))\n",
        "\n",
        "def encode_sentence(s, tokenizer):\n",
        "   tokens = list(tokenizer.tokenize(s))\n",
        "   tokens.append('[SEP]')\n",
        "  #  print(tokens)\n",
        "   return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "def bert_encode(glue_dict, tokenizer):\n",
        "  num_examples = len(glue_dict[\"sentence1\"])\n",
        "  \n",
        "  sentence1 = tf.ragged.constant([\n",
        "      encode_sentence(s, tokenizer)\n",
        "      for s in np.array(glue_dict[\"sentence1\"])])\n",
        "  sentence2 = tf.ragged.constant([\n",
        "      encode_sentence(s, tokenizer)\n",
        "       for s in np.array(glue_dict[\"sentence2\"])])\n",
        "\n",
        "  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n",
        "  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n",
        "\n",
        "  input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
        "\n",
        "  type_cls = tf.zeros_like(cls)\n",
        "  type_s1 = tf.zeros_like(sentence1)\n",
        "  type_s2 = tf.ones_like(sentence2)\n",
        "  input_type_ids = tf.concat(\n",
        "      [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n",
        "\n",
        "  inputs = {\n",
        "      'input_word_ids': input_word_ids.to_tensor(),\n",
        "      'input_mask': input_mask,\n",
        "      'input_type_ids': input_type_ids}\n",
        "\n",
        "  return inputs\n",
        "\n",
        "\n",
        "# Build the model\n",
        "# The first step is to download the configuration for the pre-trained model.\n",
        "import json\n",
        "\n",
        "bert_config_file = os.path.join(gs_folder_bert, \"bert_config.json\")\n",
        "config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\n",
        "\n",
        "bert_config = bert.configs.BertConfig.from_dict(config_dict)\n",
        "\n",
        "config_dict\n",
        "\n",
        "bert_classifier, bert_encoder = bert.bert_models.classifier_model(\n",
        "    bert_config, num_labels=6)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(model=bert_encoder)\n",
        "checkpoint.restore(\n",
        "    os.path.join(gs_folder_bert, 'bert_model.ckpt')).assert_consumed()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 30522\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f5e78ffe898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50XbEEKQ7Ur8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing data and make it in a required format\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('drive/My Drive/Training Data/data.csv', usecols=['Utterance', 'Intent'], engine='python')\n",
        "data = list()\n",
        "# print(raw_data)\n",
        "for utr, inte in zip(df[\"Utterance\"].tolist(), df[\"Intent\"].tolist()):\n",
        "    data.append((utr, inte))\n",
        "random.shuffle(data)\n",
        "# Converting into numpy array\n",
        "from numpy import array\n",
        "data = array(data)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSe3y13G76r3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "16f46c82-5cf0-4665-8108-581de2caa3c7"
      },
      "source": [
        "# splitting data into k-folds\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import operator\n",
        "\n",
        "kfold = KFold(5, True, 1)\n",
        "\n",
        "trainData = []\n",
        "testData = []\n",
        "i = 1\n",
        "for train, test in kfold.split(data):\n",
        "    print(\"Fold: \"+str(i))\n",
        "    i+=1\n",
        "    trainData = data[train].tolist()\n",
        "    testData = data[test].tolist()\n",
        "    tr = [] # list of tuple train data\n",
        "    te = [] # list of tuple test data\n",
        "    for record1 in trainData:\n",
        "      tr.append((record1[0], record1[1]))\n",
        "    for record2 in testData:\n",
        "      te.append((record2[0],record2[1]))\n",
        "      # Sorting a list intent wise\n",
        "    tr.sort(key=operator.itemgetter(1))\n",
        "    # To BERT format\n",
        "    training_data = {\"train\": {\"label\": [], \"sentence1\": [], \"sentence2\": []}}\n",
        "    testing_data = {\"test\": {\"label\": [], \"sentence1\": [], \"sentence2\": []}}\n",
        "    labels_map = {}\n",
        "   \n",
        "    for entry in tr:\n",
        "      labels_map[entry[1]] = len(labels_map) - 1\n",
        "    # print(labels_map)\n",
        "    for entry in tr:\n",
        "      training_data[\"train\"][\"label\"].append(labels_map[entry[1]])\n",
        "      training_data[\"train\"][\"sentence1\"].append(entry[0])\n",
        "      training_data[\"train\"][\"sentence2\"].append(\"\")\n",
        "    # Generating Testing Data\n",
        "    # print(labels_map)\n",
        "    # print(te)\n",
        "    for record in te:\n",
        "      testing_data[\"test\"][\"label\"].append(labels_map[record[1]])\n",
        "      testing_data[\"test\"][\"sentence1\"].append(record[0])\n",
        "      testing_data[\"test\"][\"sentence2\"].append(\"\")\n",
        "    # print(tr)\n",
        "    # print(len(tr))\n",
        "    # print(te)\n",
        "    # print(len(te))\n",
        "    \n",
        "    print(\"Training\")\n",
        "    # print(\"Training data:\" + str(training_data))\n",
        "    # Getting training data along with labels \n",
        "    train_data = bert_encode(training_data['train'], tokenizer)\n",
        "    train_labels = training_data['train']['label']\n",
        "    test_data = bert_encode(testing_data['test'], tokenizer)\n",
        "    test_labels = testing_data['test']['label']\n",
        "    # Set up epochs and steps\n",
        "    epochs = 3\n",
        "    batch_size = 8\n",
        "\n",
        "    train_data_size = len(train_labels)\n",
        "    steps_per_epoch = int(train_data_size / batch_size)\n",
        "    num_train_steps = steps_per_epoch * epochs\n",
        "    warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
        "\n",
        "    # creates an optimizer with learning rate schedule\n",
        "    optimizer = nlp.optimization.create_optimizer(\n",
        "        5e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n",
        "    \n",
        "    # Train the model\n",
        "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    bert_classifier.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=loss,\n",
        "        metrics=metrics)\n",
        "    bert_classifier.fit(\n",
        "          x=train_data, y=np.array(train_labels),\n",
        "          validation_data=(test_data,np.array(test_labels)),\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs)\n",
        "    "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold: 1\n",
            "Training\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 49s 3s/step - loss: 1.8318 - accuracy: 0.0887 - val_loss: 1.7584 - val_accuracy: 0.3500\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 32s 3s/step - loss: 1.6223 - accuracy: 0.3591 - val_loss: 1.6088 - val_accuracy: 0.4000\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 32s 3s/step - loss: 1.3769 - accuracy: 0.5631 - val_loss: 1.5755 - val_accuracy: 0.4000\n",
            "Fold: 2\n",
            "Training\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 47s 3s/step - loss: 1.3509 - accuracy: 0.5200 - val_loss: 1.2369 - val_accuracy: 0.5500\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 32s 3s/step - loss: 1.3767 - accuracy: 0.4672 - val_loss: 1.1045 - val_accuracy: 0.7500\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 30s 3s/step - loss: 0.9603 - accuracy: 0.7359 - val_loss: 1.0881 - val_accuracy: 0.7000\n",
            "Fold: 3\n",
            "Training\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 48s 3s/step - loss: 1.0977 - accuracy: 0.6336 - val_loss: 0.8647 - val_accuracy: 0.8500\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 32s 3s/step - loss: 0.9755 - accuracy: 0.8021 - val_loss: 0.8061 - val_accuracy: 0.8500\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 32s 3s/step - loss: 0.7518 - accuracy: 0.8663 - val_loss: 0.7843 - val_accuracy: 0.8000\n",
            "Fold: 4\n",
            "Training\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 48s 3s/step - loss: 0.7510 - accuracy: 0.9277 - val_loss: 0.6088 - val_accuracy: 0.9000\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 32s 3s/step - loss: 0.5763 - accuracy: 0.9875 - val_loss: 0.4890 - val_accuracy: 0.9500\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 32s 3s/step - loss: 0.5058 - accuracy: 0.9915 - val_loss: 0.4780 - val_accuracy: 0.9500\n",
            "Fold: 5\n",
            "Training\n",
            "Epoch 1/3\n",
            "10/10 [==============================] - 44s 3s/step - loss: 0.4405 - accuracy: 0.9843 - val_loss: 0.3349 - val_accuracy: 0.9474\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 32s 3s/step - loss: 0.4599 - accuracy: 0.8843 - val_loss: 0.2747 - val_accuracy: 1.0000\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 32s 3s/step - loss: 0.2527 - accuracy: 0.9929 - val_loss: 0.2889 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1qlQdZK7_RL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the model \n",
        "# bert_classifier.save('bert_classifier.h5') \n",
        "# Saving the model \n",
        "bert_classifier.save('drive/My Drive/TrainedModel/bert_classifier.h5') "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYajzSKS-iuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downloading Save model \n",
        "# from google.colab import files\n",
        "# files.download(\"/content/bert_classifier.h5\")\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHfJbn7oBLqE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "b825b217-5301-4291-e466-ee1e728add66"
      },
      "source": [
        "new_model = tf.keras.models.load_model('bert_classifier.h5')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-88c2db53dd02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert_classifier.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    183\u001b[0m           (isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[1;32m    184\u001b[0m         return hdf5_format.load_model_from_hdf5(filepath, custom_objects,\n\u001b[0;32m--> 185\u001b[0;31m                                                 compile)\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     model = model_config_lib.model_from_config(model_config,\n\u001b[0;32m--> 184\u001b[0;31m                                                custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/model_config.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                     '`Sequential.from_config(config)`?')\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m       printable_module_name='layer')\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     (cls, cls_config) = class_and_config_for_serialized_keras_object(\n\u001b[0;32m--> 347\u001b[0;31m         config, module_objects, custom_objects, printable_module_name)\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'from_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[0;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    294\u001b[0m   \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_registered_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprintable_module_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m   \u001b[0mcls_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown layer: BertClassifier"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KS0heZrBRrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "a = tf.keras.models.load_model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gQHm40gBzPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUqRKOMUCJFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from official.nlp.bert.bert_models import "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVwf6P45FnvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}